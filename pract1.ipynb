{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting page 1 to svm_basic_1.png... done!\n",
      "Converting page 2 to svm_basic_2.png... done!\n",
      "Converting page 3 to svm_basic_3.png... done!\n",
      "Converting page 4 to svm_basic_4.png... done!\n",
      "Converting page 5 to svm_basic_5.png... done!\n",
      "Converting page 6 to svm_basic_6.png... done!\n",
      "Converting page 7 to svm_basic_7.png... done!\n",
      "Converting page 8 to svm_basic_8.png... done!\n",
      "Converting page 9 to svm_basic_9.png... done!\n",
      "Converting page 10 to svm_basic_10.png... done!\n",
      "Converting page 11 to svm_basic_11.png... done!\n",
      "Converting page 12 to svm_basic_12.png... done!\n",
      "Converting page 13 to svm_basic_13.png... done!\n",
      "Converting page 14 to svm_basic_14.png... done!\n",
      "Converting page 15 to svm_basic_15.png... done!\n",
      "Converting page 16 to svm_basic_16.png... done!\n",
      "Converting page 17 to svm_basic_17.png... done!\n",
      "Converting page 18 to svm_basic_18.png... done!\n",
      "Converting page 19 to svm_basic_19.png... done!\n",
      "Converting page 20 to svm_basic_20.png... done!\n",
      "Converting page 21 to svm_basic_21.png... done!\n",
      "Converting page 22 to svm_basic_22.png... done!\n",
      "Converting page 23 to svm_basic_23.png... done!\n",
      "Converting page 24 to svm_basic_24.png... done!\n",
      "Converting page 25 to svm_basic_25.png... done!\n",
      "Converting page 26 to svm_basic_26.png... done!\n",
      "Converting page 27 to svm_basic_27.png... done!\n",
      "Converting page 28 to svm_basic_28.png... done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Support Vector Machines:\\nTheory and Applications\\nLipo Wang\\n(ed.)\\n\\nSpringer, Berlin\\n2005\\n'</li>\n",
       "\t<li>'Preface\\n\\nThe support vector machine (SVM) is a supervised learning method that\\ngenerates input-output mapping functions from a set of labeled training data.\\nThe mapping function can be either a classification function, i.e., the cate-\\ngory of the input data, or a regression function. For classification, nonlinear\\nkernel functions are often used to transform input data to a high-dimensional\\nfeature space in which the input data become more separable compared to\\nthe original input space. Maximum-margin hyperplanes are then created. ‘The\\nmodel thus produced depends on only a subset of the training data near the\\nclass boundaries. Similarly, the model produced by Support Vector Regres-\\nsion ignores any training data that is sufficiently close to the model prediction.\\nSVMs are also said to belong to “kernel methods”.\\n\\nIn addition to its solid mathematical foundation in statistical learning\\ntheory, SVMs have demonstrated highly competitive performance in numerous\\nreal-world applications, such as bioinformatics, text mining, face recognition,\\nand image processing, which has established SVMs as one of the state-of-\\nthe-art tools for machine learning and data mining, along with other soft\\ncomputing techniques, e.g., neural networks and fuzzy systems.\\n\\nThis volume is composed of 20 chapters selected from the recent myriad\\nof novel SVM applications, powerful SVM algorithms, as well as enlighten-\\ning theoretical analysis. Written by experts in their respective fields, the first\\n12 chapters concentrate on SVM theory, whereas the subsequent 8 chapters\\nemphasize practical applications, although the “decision boundary” separat-\\ning these two categories is rather “fuzzy”.\\n\\nKecman first presents an introduction on the SVM, explaining the basic\\ntheory and implementation aspects. In the chapter contributed by Ma and\\nCherkassky, a novel approach to nonlinear classification using a collection of\\nseveral simple (linear) classifiers is proposed based on a new formulation of\\nthe learning problem called multiple model estimation. Pelckmans, Goethals,\\nDe Brabanter, Suykens, and De Moor describe componentwise Least Squares\\nSupport Vector Machines (LS-SVMs) for the estimation of additive models\\nconsisting of a sum of nonlinear components.\\n'</li>\n",
       "\t<li>'VI Preface\\n\\nMotivated by the statistical query model, Mitra, Murthy and Pal study an\\nactive learning strategy to solve the large quadratic programming problem of\\nSVM design in data mining applications. Kaizhu Huang, Haiqin Yang, King,\\nand Lyu propose a unifying theory of the Maxi-Min Margin Machine (M4)\\nthat subsumes the SVM, the minimax probability machine, and the linear\\ndiscriminant analysis. Vogt and Kecman present an active-set algorithm for\\nquadratic programming problems in SVMs, as an alternative to working-set\\n(decomposition) techniques, especially when the data set is not too large, the\\nproblem is ill-conditioned, or when high precision is needed.\\n\\nBeing aware of the abundance of methods for SVM model selection,\\nAnguita, Boni, Ridella, Rivieccio, and Sterpi carefully analyze the most well-\\nknown methods and test some of them on standard benchmarks to evaluate\\ntheir effectiveness. In an attempt to minimize bias, Peng, Heisterkamp, and\\nDai propose locally adaptive nearest neighbor classification methods by using\\nlocally linear SVMs and quasiconformal transformed kernels. Williams, Wu,\\nand Feng discuss two geometric methods to improve SVM performance, i.e.,\\n(1) adapting kernels by magnifying the Riemannian metric in the neighbor-\\nhood of the boundary, thereby increasing class separation, and (2) optimally\\nlocating the separating boundary, given that the distributions of data on either\\nside may have different scales.\\n\\nSong, Hu, and Xulei Yang derive a Kuhn-Tucker condition and a decom-\\nposition algorithm for robust SVMs to deal with overfitting in the presence of\\noutliers. Lin and Sheng-de Wang design a fuzzy SVM with automatic deter-\\nmination of the membership functions. Kecman, Te-Ming Huang, and Vogt\\npresent the latest developments and results of the Iterative Single Data Algo-\\nrithm for solving large-scale problems.\\n\\nExploiting regularization and subspace decomposition techniques, Lu,\\nPlataniotis, and Venetsanopoulos introduce a new kernel discriminant learn-\\ning method and apply the method to face recognition. Kwang In Kim, Jung,\\nand Hang Joon Kim employ SVMs and neural networks for automobile li-\\ncense plate localization, by classifying each pixel in the image into the object\\nof interest or the background based on localized color texture patterns. Mat-\\ntera discusses SVM applications in signal processing, especially the problem\\nof digital channel equalization. Chu, Jin, and Lipo Wang use SVMs to solve\\ntwo important problems in bioinformatics, i.e., cancer diagnosis based on mi-\\ncroarray gene expression data and protein secondary structure prediction.\\n\\nEmulating the natural nose, Brezmes, Llobet, Al-Khalifa, Maldonado, and\\nGardner describe how SVMs are being evaluated in the gas sensor commu-\\nnity to discriminate different blends of coffee, different types of vapors and\\nnerve agents. Zhan presents an application of the SVM in inverse problems\\nin ocean color remote sensing. Liang uses SVMs for non-invasive diagnosis\\nof delayed gastric emptying from the cutaneous electrogastrograms (EGGs).\\nRojo-Alvarez, Garcia-Alberola, Artés-Rodriguez, and Arenal-Maiz apply\\nSVMs, together with bootstrap resampling and principal component analysis,\\nto tachycardia discrimination in implantable cardioverter defibrillators.\\n'</li>\n",
       "\t<li>'Preface Vil\\nI would like to express my sincere appreciation to all authors and reviewers\\nwho have spent their precious time and efforts in making this book a reality.\\nI wish to especially thank Professor Vojislav Kecman, who graciously took\\non the enormous task of writing a comprehensive introductory chapter, in\\naddition to his other great contributions to this book. My gratitude also goes\\nto Professor Janusz Kacprzyk and Dr. Thomas Ditzinger for their kindest\\nsupport and help with this book.\\nSingapore Lipo Wang\\nJanuary 2005\\n'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'Contents\\n\\nSupport Vector Machines — An Introduction\\n\\nV. K€CMGN. 6. eee eee eee eee eeeeeee\\nMultiple Model Estimation\\n\\nfor Nonlinear Classification\\n\\nY. Ma and V. Cherkassky .. 0.0.00. AY\\nComponentwise Least Squares Support Vector Machines\\n\\nKk. Pelckmans, I. Goethals, J. De Brabanter, J.A.K. Suykens, and B.\\n\\nDe MOor .. ccc nee eee eee eee TT\\nActive Support Vector Learning with Statistical Queries\\n\\nP. Mitra, C.A. Murthy, and SK. Pal... eee ee ID\\nLocal Learning vs. Global Learning: An Introduction\\n\\nto Maxi-Min Margin Machine\\n\\nKk. Huang, H. Yang, I. King, and M.R. Lyu ..... eee eee ee ee L18\\nActive-Set Methods for Support Vector Machines\\n\\nM. Vogt and V. Kecman ... 6... cece ee eee ee A 138\\nTheoretical and Practical Model Selection Methods\\n\\nfor Support Vector Classifiers\\n\\nD. Anguita, A. Boni, S. Ridella, FP. Rivieccio, and D. Sterpi...........159\\nAdaptive Discriminant\\n\\nand Quasiconformal Kernel Nearest Neighbor Classification\\n\\nJ. Peng, D.R. Heisterkamp, and H.K. Dat .. 1... ee ee LB\\nImproving the Performance of the Support Vector Machine:\\n\\nTwo Geometrical Scaling Methods\\n\\nP. Williams, S. Wu, and J. Feng 0.0... ccc cee eee ee 205\\n'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Support Vector Machines:\\textbackslash{}nTheory and Applications\\textbackslash{}nLipo Wang\\textbackslash{}n(ed.)\\textbackslash{}n\\textbackslash{}nSpringer, Berlin\\textbackslash{}n2005\\textbackslash{}n'\n",
       "\\item 'Preface\\textbackslash{}n\\textbackslash{}nThe support vector machine (SVM) is a supervised learning method that\\textbackslash{}ngenerates input-output mapping functions from a set of labeled training data.\\textbackslash{}nThe mapping function can be either a classification function, i.e., the cate-\\textbackslash{}ngory of the input data, or a regression function. For classification, nonlinear\\textbackslash{}nkernel functions are often used to transform input data to a high-dimensional\\textbackslash{}nfeature space in which the input data become more separable compared to\\textbackslash{}nthe original input space. Maximum-margin hyperplanes are then created. ‘The\\textbackslash{}nmodel thus produced depends on only a subset of the training data near the\\textbackslash{}nclass boundaries. Similarly, the model produced by Support Vector Regres-\\textbackslash{}nsion ignores any training data that is sufficiently close to the model prediction.\\textbackslash{}nSVMs are also said to belong to “kernel methods”.\\textbackslash{}n\\textbackslash{}nIn addition to its solid mathematical foundation in statistical learning\\textbackslash{}ntheory, SVMs have demonstrated highly competitive performance in numerous\\textbackslash{}nreal-world applications, such as bioinformatics, text mining, face recognition,\\textbackslash{}nand image processing, which has established SVMs as one of the state-of-\\textbackslash{}nthe-art tools for machine learning and data mining, along with other soft\\textbackslash{}ncomputing techniques, e.g., neural networks and fuzzy systems.\\textbackslash{}n\\textbackslash{}nThis volume is composed of 20 chapters selected from the recent myriad\\textbackslash{}nof novel SVM applications, powerful SVM algorithms, as well as enlighten-\\textbackslash{}ning theoretical analysis. Written by experts in their respective fields, the first\\textbackslash{}n12 chapters concentrate on SVM theory, whereas the subsequent 8 chapters\\textbackslash{}nemphasize practical applications, although the “decision boundary” separat-\\textbackslash{}ning these two categories is rather “fuzzy”.\\textbackslash{}n\\textbackslash{}nKecman first presents an introduction on the SVM, explaining the basic\\textbackslash{}ntheory and implementation aspects. In the chapter contributed by Ma and\\textbackslash{}nCherkassky, a novel approach to nonlinear classification using a collection of\\textbackslash{}nseveral simple (linear) classifiers is proposed based on a new formulation of\\textbackslash{}nthe learning problem called multiple model estimation. Pelckmans, Goethals,\\textbackslash{}nDe Brabanter, Suykens, and De Moor describe componentwise Least Squares\\textbackslash{}nSupport Vector Machines (LS-SVMs) for the estimation of additive models\\textbackslash{}nconsisting of a sum of nonlinear components.\\textbackslash{}n'\n",
       "\\item 'VI Preface\\textbackslash{}n\\textbackslash{}nMotivated by the statistical query model, Mitra, Murthy and Pal study an\\textbackslash{}nactive learning strategy to solve the large quadratic programming problem of\\textbackslash{}nSVM design in data mining applications. Kaizhu Huang, Haiqin Yang, King,\\textbackslash{}nand Lyu propose a unifying theory of the Maxi-Min Margin Machine (M4)\\textbackslash{}nthat subsumes the SVM, the minimax probability machine, and the linear\\textbackslash{}ndiscriminant analysis. Vogt and Kecman present an active-set algorithm for\\textbackslash{}nquadratic programming problems in SVMs, as an alternative to working-set\\textbackslash{}n(decomposition) techniques, especially when the data set is not too large, the\\textbackslash{}nproblem is ill-conditioned, or when high precision is needed.\\textbackslash{}n\\textbackslash{}nBeing aware of the abundance of methods for SVM model selection,\\textbackslash{}nAnguita, Boni, Ridella, Rivieccio, and Sterpi carefully analyze the most well-\\textbackslash{}nknown methods and test some of them on standard benchmarks to evaluate\\textbackslash{}ntheir effectiveness. In an attempt to minimize bias, Peng, Heisterkamp, and\\textbackslash{}nDai propose locally adaptive nearest neighbor classification methods by using\\textbackslash{}nlocally linear SVMs and quasiconformal transformed kernels. Williams, Wu,\\textbackslash{}nand Feng discuss two geometric methods to improve SVM performance, i.e.,\\textbackslash{}n(1) adapting kernels by magnifying the Riemannian metric in the neighbor-\\textbackslash{}nhood of the boundary, thereby increasing class separation, and (2) optimally\\textbackslash{}nlocating the separating boundary, given that the distributions of data on either\\textbackslash{}nside may have different scales.\\textbackslash{}n\\textbackslash{}nSong, Hu, and Xulei Yang derive a Kuhn-Tucker condition and a decom-\\textbackslash{}nposition algorithm for robust SVMs to deal with overfitting in the presence of\\textbackslash{}noutliers. Lin and Sheng-de Wang design a fuzzy SVM with automatic deter-\\textbackslash{}nmination of the membership functions. Kecman, Te-Ming Huang, and Vogt\\textbackslash{}npresent the latest developments and results of the Iterative Single Data Algo-\\textbackslash{}nrithm for solving large-scale problems.\\textbackslash{}n\\textbackslash{}nExploiting regularization and subspace decomposition techniques, Lu,\\textbackslash{}nPlataniotis, and Venetsanopoulos introduce a new kernel discriminant learn-\\textbackslash{}ning method and apply the method to face recognition. Kwang In Kim, Jung,\\textbackslash{}nand Hang Joon Kim employ SVMs and neural networks for automobile li-\\textbackslash{}ncense plate localization, by classifying each pixel in the image into the object\\textbackslash{}nof interest or the background based on localized color texture patterns. Mat-\\textbackslash{}ntera discusses SVM applications in signal processing, especially the problem\\textbackslash{}nof digital channel equalization. Chu, Jin, and Lipo Wang use SVMs to solve\\textbackslash{}ntwo important problems in bioinformatics, i.e., cancer diagnosis based on mi-\\textbackslash{}ncroarray gene expression data and protein secondary structure prediction.\\textbackslash{}n\\textbackslash{}nEmulating the natural nose, Brezmes, Llobet, Al-Khalifa, Maldonado, and\\textbackslash{}nGardner describe how SVMs are being evaluated in the gas sensor commu-\\textbackslash{}nnity to discriminate different blends of coffee, different types of vapors and\\textbackslash{}nnerve agents. Zhan presents an application of the SVM in inverse problems\\textbackslash{}nin ocean color remote sensing. Liang uses SVMs for non-invasive diagnosis\\textbackslash{}nof delayed gastric emptying from the cutaneous electrogastrograms (EGGs).\\textbackslash{}nRojo-Alvarez, Garcia-Alberola, Artés-Rodriguez, and Arenal-Maiz apply\\textbackslash{}nSVMs, together with bootstrap resampling and principal component analysis,\\textbackslash{}nto tachycardia discrimination in implantable cardioverter defibrillators.\\textbackslash{}n'\n",
       "\\item 'Preface Vil\\textbackslash{}nI would like to express my sincere appreciation to all authors and reviewers\\textbackslash{}nwho have spent their precious time and efforts in making this book a reality.\\textbackslash{}nI wish to especially thank Professor Vojislav Kecman, who graciously took\\textbackslash{}non the enormous task of writing a comprehensive introductory chapter, in\\textbackslash{}naddition to his other great contributions to this book. My gratitude also goes\\textbackslash{}nto Professor Janusz Kacprzyk and Dr. Thomas Ditzinger for their kindest\\textbackslash{}nsupport and help with this book.\\textbackslash{}nSingapore Lipo Wang\\textbackslash{}nJanuary 2005\\textbackslash{}n'\n",
       "\\item ''\n",
       "\\item 'Contents\\textbackslash{}n\\textbackslash{}nSupport Vector Machines — An Introduction\\textbackslash{}n\\textbackslash{}nV. K€CMGN. 6. eee eee eee eee eeeeeee\\textbackslash{}nMultiple Model Estimation\\textbackslash{}n\\textbackslash{}nfor Nonlinear Classification\\textbackslash{}n\\textbackslash{}nY. Ma and V. Cherkassky .. 0.0.00. AY\\textbackslash{}nComponentwise Least Squares Support Vector Machines\\textbackslash{}n\\textbackslash{}nKk. Pelckmans, I. Goethals, J. De Brabanter, J.A.K. Suykens, and B.\\textbackslash{}n\\textbackslash{}nDe MOor .. ccc nee eee eee eee TT\\textbackslash{}nActive Support Vector Learning with Statistical Queries\\textbackslash{}n\\textbackslash{}nP. Mitra, C.A. Murthy, and SK. Pal... eee ee ID\\textbackslash{}nLocal Learning vs. Global Learning: An Introduction\\textbackslash{}n\\textbackslash{}nto Maxi-Min Margin Machine\\textbackslash{}n\\textbackslash{}nKk. Huang, H. Yang, I. King, and M.R. Lyu ..... eee eee ee ee L18\\textbackslash{}nActive-Set Methods for Support Vector Machines\\textbackslash{}n\\textbackslash{}nM. Vogt and V. Kecman ... 6... cece ee eee ee A 138\\textbackslash{}nTheoretical and Practical Model Selection Methods\\textbackslash{}n\\textbackslash{}nfor Support Vector Classifiers\\textbackslash{}n\\textbackslash{}nD. Anguita, A. Boni, S. Ridella, FP. Rivieccio, and D. Sterpi...........159\\textbackslash{}nAdaptive Discriminant\\textbackslash{}n\\textbackslash{}nand Quasiconformal Kernel Nearest Neighbor Classification\\textbackslash{}n\\textbackslash{}nJ. Peng, D.R. Heisterkamp, and H.K. Dat .. 1... ee ee LB\\textbackslash{}nImproving the Performance of the Support Vector Machine:\\textbackslash{}n\\textbackslash{}nTwo Geometrical Scaling Methods\\textbackslash{}n\\textbackslash{}nP. Williams, S. Wu, and J. Feng 0.0... ccc cee eee ee 205\\textbackslash{}n'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Support Vector Machines:\\nTheory and Applications\\nLipo Wang\\n(ed.)\\n\\nSpringer, Berlin\\n2005\\n'\n",
       "2. 'Preface\\n\\nThe support vector machine (SVM) is a supervised learning method that\\ngenerates input-output mapping functions from a set of labeled training data.\\nThe mapping function can be either a classification function, i.e., the cate-\\ngory of the input data, or a regression function. For classification, nonlinear\\nkernel functions are often used to transform input data to a high-dimensional\\nfeature space in which the input data become more separable compared to\\nthe original input space. Maximum-margin hyperplanes are then created. ‘The\\nmodel thus produced depends on only a subset of the training data near the\\nclass boundaries. Similarly, the model produced by Support Vector Regres-\\nsion ignores any training data that is sufficiently close to the model prediction.\\nSVMs are also said to belong to “kernel methods”.\\n\\nIn addition to its solid mathematical foundation in statistical learning\\ntheory, SVMs have demonstrated highly competitive performance in numerous\\nreal-world applications, such as bioinformatics, text mining, face recognition,\\nand image processing, which has established SVMs as one of the state-of-\\nthe-art tools for machine learning and data mining, along with other soft\\ncomputing techniques, e.g., neural networks and fuzzy systems.\\n\\nThis volume is composed of 20 chapters selected from the recent myriad\\nof novel SVM applications, powerful SVM algorithms, as well as enlighten-\\ning theoretical analysis. Written by experts in their respective fields, the first\\n12 chapters concentrate on SVM theory, whereas the subsequent 8 chapters\\nemphasize practical applications, although the “decision boundary” separat-\\ning these two categories is rather “fuzzy”.\\n\\nKecman first presents an introduction on the SVM, explaining the basic\\ntheory and implementation aspects. In the chapter contributed by Ma and\\nCherkassky, a novel approach to nonlinear classification using a collection of\\nseveral simple (linear) classifiers is proposed based on a new formulation of\\nthe learning problem called multiple model estimation. Pelckmans, Goethals,\\nDe Brabanter, Suykens, and De Moor describe componentwise Least Squares\\nSupport Vector Machines (LS-SVMs) for the estimation of additive models\\nconsisting of a sum of nonlinear components.\\n'\n",
       "3. 'VI Preface\\n\\nMotivated by the statistical query model, Mitra, Murthy and Pal study an\\nactive learning strategy to solve the large quadratic programming problem of\\nSVM design in data mining applications. Kaizhu Huang, Haiqin Yang, King,\\nand Lyu propose a unifying theory of the Maxi-Min Margin Machine (M4)\\nthat subsumes the SVM, the minimax probability machine, and the linear\\ndiscriminant analysis. Vogt and Kecman present an active-set algorithm for\\nquadratic programming problems in SVMs, as an alternative to working-set\\n(decomposition) techniques, especially when the data set is not too large, the\\nproblem is ill-conditioned, or when high precision is needed.\\n\\nBeing aware of the abundance of methods for SVM model selection,\\nAnguita, Boni, Ridella, Rivieccio, and Sterpi carefully analyze the most well-\\nknown methods and test some of them on standard benchmarks to evaluate\\ntheir effectiveness. In an attempt to minimize bias, Peng, Heisterkamp, and\\nDai propose locally adaptive nearest neighbor classification methods by using\\nlocally linear SVMs and quasiconformal transformed kernels. Williams, Wu,\\nand Feng discuss two geometric methods to improve SVM performance, i.e.,\\n(1) adapting kernels by magnifying the Riemannian metric in the neighbor-\\nhood of the boundary, thereby increasing class separation, and (2) optimally\\nlocating the separating boundary, given that the distributions of data on either\\nside may have different scales.\\n\\nSong, Hu, and Xulei Yang derive a Kuhn-Tucker condition and a decom-\\nposition algorithm for robust SVMs to deal with overfitting in the presence of\\noutliers. Lin and Sheng-de Wang design a fuzzy SVM with automatic deter-\\nmination of the membership functions. Kecman, Te-Ming Huang, and Vogt\\npresent the latest developments and results of the Iterative Single Data Algo-\\nrithm for solving large-scale problems.\\n\\nExploiting regularization and subspace decomposition techniques, Lu,\\nPlataniotis, and Venetsanopoulos introduce a new kernel discriminant learn-\\ning method and apply the method to face recognition. Kwang In Kim, Jung,\\nand Hang Joon Kim employ SVMs and neural networks for automobile li-\\ncense plate localization, by classifying each pixel in the image into the object\\nof interest or the background based on localized color texture patterns. Mat-\\ntera discusses SVM applications in signal processing, especially the problem\\nof digital channel equalization. Chu, Jin, and Lipo Wang use SVMs to solve\\ntwo important problems in bioinformatics, i.e., cancer diagnosis based on mi-\\ncroarray gene expression data and protein secondary structure prediction.\\n\\nEmulating the natural nose, Brezmes, Llobet, Al-Khalifa, Maldonado, and\\nGardner describe how SVMs are being evaluated in the gas sensor commu-\\nnity to discriminate different blends of coffee, different types of vapors and\\nnerve agents. Zhan presents an application of the SVM in inverse problems\\nin ocean color remote sensing. Liang uses SVMs for non-invasive diagnosis\\nof delayed gastric emptying from the cutaneous electrogastrograms (EGGs).\\nRojo-Alvarez, Garcia-Alberola, Artés-Rodriguez, and Arenal-Maiz apply\\nSVMs, together with bootstrap resampling and principal component analysis,\\nto tachycardia discrimination in implantable cardioverter defibrillators.\\n'\n",
       "4. 'Preface Vil\\nI would like to express my sincere appreciation to all authors and reviewers\\nwho have spent their precious time and efforts in making this book a reality.\\nI wish to especially thank Professor Vojislav Kecman, who graciously took\\non the enormous task of writing a comprehensive introductory chapter, in\\naddition to his other great contributions to this book. My gratitude also goes\\nto Professor Janusz Kacprzyk and Dr. Thomas Ditzinger for their kindest\\nsupport and help with this book.\\nSingapore Lipo Wang\\nJanuary 2005\\n'\n",
       "5. ''\n",
       "6. 'Contents\\n\\nSupport Vector Machines — An Introduction\\n\\nV. K€CMGN. 6. eee eee eee eee eeeeeee\\nMultiple Model Estimation\\n\\nfor Nonlinear Classification\\n\\nY. Ma and V. Cherkassky .. 0.0.00. AY\\nComponentwise Least Squares Support Vector Machines\\n\\nKk. Pelckmans, I. Goethals, J. De Brabanter, J.A.K. Suykens, and B.\\n\\nDe MOor .. ccc nee eee eee eee TT\\nActive Support Vector Learning with Statistical Queries\\n\\nP. Mitra, C.A. Murthy, and SK. Pal... eee ee ID\\nLocal Learning vs. Global Learning: An Introduction\\n\\nto Maxi-Min Margin Machine\\n\\nKk. Huang, H. Yang, I. King, and M.R. Lyu ..... eee eee ee ee L18\\nActive-Set Methods for Support Vector Machines\\n\\nM. Vogt and V. Kecman ... 6... cece ee eee ee A 138\\nTheoretical and Practical Model Selection Methods\\n\\nfor Support Vector Classifiers\\n\\nD. Anguita, A. Boni, S. Ridella, FP. Rivieccio, and D. Sterpi...........159\\nAdaptive Discriminant\\n\\nand Quasiconformal Kernel Nearest Neighbor Classification\\n\\nJ. Peng, D.R. Heisterkamp, and H.K. Dat .. 1... ee ee LB\\nImproving the Performance of the Support Vector Machine:\\n\\nTwo Geometrical Scaling Methods\\n\\nP. Williams, S. Wu, and J. Feng 0.0... ccc cee eee ee 205\\n'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"Support Vector Machines:\\nTheory and Applications\\nLipo Wang\\n(ed.)\\n\\nSpringer, Berlin\\n2005\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "[2] \"Preface\\n\\nThe support vector machine (SVM) is a supervised learning method that\\ngenerates input-output mapping functions from a set of labeled training data.\\nThe mapping function can be either a classification function, i.e., the cate-\\ngory of the input data, or a regression function. For classification, nonlinear\\nkernel functions are often used to transform input data to a high-dimensional\\nfeature space in which the input data become more separable compared to\\nthe original input space. Maximum-margin hyperplanes are then created. ‘The\\nmodel thus produced depends on only a subset of the training data near the\\nclass boundaries. Similarly, the model produced by Support Vector Regres-\\nsion ignores any training data that is sufficiently close to the model prediction.\\nSVMs are also said to belong to “kernel methods”.\\n\\nIn addition to its solid mathematical foundation in statistical learning\\ntheory, SVMs have demonstrated highly competitive performance in numerous\\nreal-world applications, such as bioinformatics, text mining, face recognition,\\nand image processing, which has established SVMs as one of the state-of-\\nthe-art tools for machine learning and data mining, along with other soft\\ncomputing techniques, e.g., neural networks and fuzzy systems.\\n\\nThis volume is composed of 20 chapters selected from the recent myriad\\nof novel SVM applications, powerful SVM algorithms, as well as enlighten-\\ning theoretical analysis. Written by experts in their respective fields, the first\\n12 chapters concentrate on SVM theory, whereas the subsequent 8 chapters\\nemphasize practical applications, although the “decision boundary” separat-\\ning these two categories is rather “fuzzy”.\\n\\nKecman first presents an introduction on the SVM, explaining the basic\\ntheory and implementation aspects. In the chapter contributed by Ma and\\nCherkassky, a novel approach to nonlinear classification using a collection of\\nseveral simple (linear) classifiers is proposed based on a new formulation of\\nthe learning problem called multiple model estimation. Pelckmans, Goethals,\\nDe Brabanter, Suykens, and De Moor describe componentwise Least Squares\\nSupport Vector Machines (LS-SVMs) for the estimation of additive models\\nconsisting of a sum of nonlinear components.\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "[3] \"VI Preface\\n\\nMotivated by the statistical query model, Mitra, Murthy and Pal study an\\nactive learning strategy to solve the large quadratic programming problem of\\nSVM design in data mining applications. Kaizhu Huang, Haiqin Yang, King,\\nand Lyu propose a unifying theory of the Maxi-Min Margin Machine (M4)\\nthat subsumes the SVM, the minimax probability machine, and the linear\\ndiscriminant analysis. Vogt and Kecman present an active-set algorithm for\\nquadratic programming problems in SVMs, as an alternative to working-set\\n(decomposition) techniques, especially when the data set is not too large, the\\nproblem is ill-conditioned, or when high precision is needed.\\n\\nBeing aware of the abundance of methods for SVM model selection,\\nAnguita, Boni, Ridella, Rivieccio, and Sterpi carefully analyze the most well-\\nknown methods and test some of them on standard benchmarks to evaluate\\ntheir effectiveness. In an attempt to minimize bias, Peng, Heisterkamp, and\\nDai propose locally adaptive nearest neighbor classification methods by using\\nlocally linear SVMs and quasiconformal transformed kernels. Williams, Wu,\\nand Feng discuss two geometric methods to improve SVM performance, i.e.,\\n(1) adapting kernels by magnifying the Riemannian metric in the neighbor-\\nhood of the boundary, thereby increasing class separation, and (2) optimally\\nlocating the separating boundary, given that the distributions of data on either\\nside may have different scales.\\n\\nSong, Hu, and Xulei Yang derive a Kuhn-Tucker condition and a decom-\\nposition algorithm for robust SVMs to deal with overfitting in the presence of\\noutliers. Lin and Sheng-de Wang design a fuzzy SVM with automatic deter-\\nmination of the membership functions. Kecman, Te-Ming Huang, and Vogt\\npresent the latest developments and results of the Iterative Single Data Algo-\\nrithm for solving large-scale problems.\\n\\nExploiting regularization and subspace decomposition techniques, Lu,\\nPlataniotis, and Venetsanopoulos introduce a new kernel discriminant learn-\\ning method and apply the method to face recognition. Kwang In Kim, Jung,\\nand Hang Joon Kim employ SVMs and neural networks for automobile li-\\ncense plate localization, by classifying each pixel in the image into the object\\nof interest or the background based on localized color texture patterns. Mat-\\ntera discusses SVM applications in signal processing, especially the problem\\nof digital channel equalization. Chu, Jin, and Lipo Wang use SVMs to solve\\ntwo important problems in bioinformatics, i.e., cancer diagnosis based on mi-\\ncroarray gene expression data and protein secondary structure prediction.\\n\\nEmulating the natural nose, Brezmes, Llobet, Al-Khalifa, Maldonado, and\\nGardner describe how SVMs are being evaluated in the gas sensor commu-\\nnity to discriminate different blends of coffee, different types of vapors and\\nnerve agents. Zhan presents an application of the SVM in inverse problems\\nin ocean color remote sensing. Liang uses SVMs for non-invasive diagnosis\\nof delayed gastric emptying from the cutaneous electrogastrograms (EGGs).\\nRojo-Alvarez, Garcia-Alberola, Artés-Rodriguez, and Arenal-Maiz apply\\nSVMs, together with bootstrap resampling and principal component analysis,\\nto tachycardia discrimination in implantable cardioverter defibrillators.\\n\"\n",
       "[4] \"Preface Vil\\nI would like to express my sincere appreciation to all authors and reviewers\\nwho have spent their precious time and efforts in making this book a reality.\\nI wish to especially thank Professor Vojislav Kecman, who graciously took\\non the enormous task of writing a comprehensive introductory chapter, in\\naddition to his other great contributions to this book. My gratitude also goes\\nto Professor Janusz Kacprzyk and Dr. Thomas Ditzinger for their kindest\\nsupport and help with this book.\\nSingapore Lipo Wang\\nJanuary 2005\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "[5] \"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "[6] \"Contents\\n\\nSupport Vector Machines — An Introduction\\n\\nV. K\\200CMGN. 6. eee eee eee eee eeeeeee\\nMultiple Model Estimation\\n\\nfor Nonlinear Classification\\n\\nY. Ma and V. Cherkassky .. 0.0.00. AY\\nComponentwise Least Squares Support Vector Machines\\n\\nKk. Pelckmans, I. Goethals, J. De Brabanter, J.A.K. Suykens, and B.\\n\\nDe MOor .. ccc nee eee eee eee TT\\nActive Support Vector Learning with Statistical Queries\\n\\nP. Mitra, C.A. Murthy, and SK. Pal... eee ee ID\\nLocal Learning vs. Global Learning: An Introduction\\n\\nto Maxi-Min Margin Machine\\n\\nKk. Huang, H. Yang, I. King, and M.R. Lyu ..... eee eee ee ee L18\\nActive-Set Methods for Support Vector Machines\\n\\nM. Vogt and V. Kecman ... 6... cece ee eee ee A 138\\nTheoretical and Practical Model Selection Methods\\n\\nfor Support Vector Classifiers\\n\\nD. Anguita, A. Boni, S. Ridella, FP. Rivieccio, and D. Sterpi...........159\\nAdaptive Discriminant\\n\\nand Quasiconformal Kernel Nearest Neighbor Classification\\n\\nJ. Peng, D.R. Heisterkamp, and H.K. Dat .. 1... ee ee LB\\nImproving the Performance of the Support Vector Machine:\\n\\nTwo Geometrical Scaling Methods\\n\\nP. Williams, S. Wu, and J. Feng 0.0... ccc cee eee ee 205\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(tesseract)\n",
    "library(magick)\n",
    "\n",
    "pngfile <- pdftools::pdf_convert(\"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\svm_basic.pdf\", dpi=600)\n",
    "\n",
    "text <- tesseract::ocr(pngfile)\n",
    "head(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'Support Vector Machines:'</li>\n",
       "\t<li>'Theory and Applications'</li>\n",
       "\t<li>'Lipo Wang'</li>\n",
       "\t<li>'(ed.)'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'Springer, Berlin'</li>\n",
       "\t<li>'2005'</li>\n",
       "</ol>\n",
       "</li>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'Preface'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'The support vector machine (SVM) is a supervised learning method that'</li>\n",
       "\t<li>'generates input-output mapping functions from a set of labeled training data.'</li>\n",
       "\t<li>'The mapping function can be either a classification function, i.e., the cate-'</li>\n",
       "\t<li>'gory of the input data, or a regression function. For classification, nonlinear'</li>\n",
       "\t<li>'kernel functions are often used to transform input data to a high-dimensional'</li>\n",
       "\t<li>'feature space in which the input data become more separable compared to'</li>\n",
       "\t<li>'the original input space. Maximum-margin hyperplanes are then created. ‘The'</li>\n",
       "\t<li>'model thus produced depends on only a subset of the training data near the'</li>\n",
       "\t<li>'class boundaries. Similarly, the model produced by Support Vector Regres-'</li>\n",
       "\t<li>'sion ignores any training data that is sufficiently close to the model prediction.'</li>\n",
       "\t<li>'SVMs are also said to belong to “kernel methods”.'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'In addition to its solid mathematical foundation in statistical learning'</li>\n",
       "\t<li>'theory, SVMs have demonstrated highly competitive performance in numerous'</li>\n",
       "\t<li>'real-world applications, such as bioinformatics, text mining, face recognition,'</li>\n",
       "\t<li>'and image processing, which has established SVMs as one of the state-of-'</li>\n",
       "\t<li>'the-art tools for machine learning and data mining, along with other soft'</li>\n",
       "\t<li>'computing techniques, e.g., neural networks and fuzzy systems.'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'This volume is composed of 20 chapters selected from the recent myriad'</li>\n",
       "\t<li>'of novel SVM applications, powerful SVM algorithms, as well as enlighten-'</li>\n",
       "\t<li>'ing theoretical analysis. Written by experts in their respective fields, the first'</li>\n",
       "\t<li>'12 chapters concentrate on SVM theory, whereas the subsequent 8 chapters'</li>\n",
       "\t<li>'emphasize practical applications, although the “decision boundary” separat-'</li>\n",
       "\t<li>'ing these two categories is rather “fuzzy”.'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'Kecman first presents an introduction on the SVM, explaining the basic'</li>\n",
       "\t<li>'theory and implementation aspects. In the chapter contributed by Ma and'</li>\n",
       "\t<li>'Cherkassky, a novel approach to nonlinear classification using a collection of'</li>\n",
       "\t<li>'several simple (linear) classifiers is proposed based on a new formulation of'</li>\n",
       "\t<li>'the learning problem called multiple model estimation. Pelckmans, Goethals,'</li>\n",
       "\t<li>'De Brabanter, Suykens, and De Moor describe componentwise Least Squares'</li>\n",
       "\t<li>'Support Vector Machines (LS-SVMs) for the estimation of additive models'</li>\n",
       "\t<li>'consisting of a sum of nonlinear components.'</li>\n",
       "</ol>\n",
       "</li>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'VI Preface'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'Motivated by the statistical query model, Mitra, Murthy and Pal study an'</li>\n",
       "\t<li>'active learning strategy to solve the large quadratic programming problem of'</li>\n",
       "\t<li>'SVM design in data mining applications. Kaizhu Huang, Haiqin Yang, King,'</li>\n",
       "\t<li>'and Lyu propose a unifying theory of the Maxi-Min Margin Machine (M4)'</li>\n",
       "\t<li>'that subsumes the SVM, the minimax probability machine, and the linear'</li>\n",
       "\t<li>'discriminant analysis. Vogt and Kecman present an active-set algorithm for'</li>\n",
       "\t<li>'quadratic programming problems in SVMs, as an alternative to working-set'</li>\n",
       "\t<li>'(decomposition) techniques, especially when the data set is not too large, the'</li>\n",
       "\t<li>'problem is ill-conditioned, or when high precision is needed.'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'Being aware of the abundance of methods for SVM model selection,'</li>\n",
       "\t<li>'Anguita, Boni, Ridella, Rivieccio, and Sterpi carefully analyze the most well-'</li>\n",
       "\t<li>'known methods and test some of them on standard benchmarks to evaluate'</li>\n",
       "\t<li>'their effectiveness. In an attempt to minimize bias, Peng, Heisterkamp, and'</li>\n",
       "\t<li>'Dai propose locally adaptive nearest neighbor classification methods by using'</li>\n",
       "\t<li>'locally linear SVMs and quasiconformal transformed kernels. Williams, Wu,'</li>\n",
       "\t<li>'and Feng discuss two geometric methods to improve SVM performance, i.e.,'</li>\n",
       "\t<li>'(1) adapting kernels by magnifying the Riemannian metric in the neighbor-'</li>\n",
       "\t<li>'hood of the boundary, thereby increasing class separation, and (2) optimally'</li>\n",
       "\t<li>'locating the separating boundary, given that the distributions of data on either'</li>\n",
       "\t<li>'side may have different scales.'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'Song, Hu, and Xulei Yang derive a Kuhn-Tucker condition and a decom-'</li>\n",
       "\t<li>'position algorithm for robust SVMs to deal with overfitting in the presence of'</li>\n",
       "\t<li>'outliers. Lin and Sheng-de Wang design a fuzzy SVM with automatic deter-'</li>\n",
       "\t<li>'mination of the membership functions. Kecman, Te-Ming Huang, and Vogt'</li>\n",
       "\t<li>'present the latest developments and results of the Iterative Single Data Algo-'</li>\n",
       "\t<li>'rithm for solving large-scale problems.'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'Exploiting regularization and subspace decomposition techniques, Lu,'</li>\n",
       "\t<li>'Plataniotis, and Venetsanopoulos introduce a new kernel discriminant learn-'</li>\n",
       "\t<li>'ing method and apply the method to face recognition. Kwang In Kim, Jung,'</li>\n",
       "\t<li>'and Hang Joon Kim employ SVMs and neural networks for automobile li-'</li>\n",
       "\t<li>'cense plate localization, by classifying each pixel in the image into the object'</li>\n",
       "\t<li>'of interest or the background based on localized color texture patterns. Mat-'</li>\n",
       "\t<li>'tera discusses SVM applications in signal processing, especially the problem'</li>\n",
       "\t<li>'of digital channel equalization. Chu, Jin, and Lipo Wang use SVMs to solve'</li>\n",
       "\t<li>'two important problems in bioinformatics, i.e., cancer diagnosis based on mi-'</li>\n",
       "\t<li>'croarray gene expression data and protein secondary structure prediction.'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'Emulating the natural nose, Brezmes, Llobet, Al-Khalifa, Maldonado, and'</li>\n",
       "\t<li>'Gardner describe how SVMs are being evaluated in the gas sensor commu-'</li>\n",
       "\t<li>'nity to discriminate different blends of coffee, different types of vapors and'</li>\n",
       "\t<li>'nerve agents. Zhan presents an application of the SVM in inverse problems'</li>\n",
       "\t<li>'in ocean color remote sensing. Liang uses SVMs for non-invasive diagnosis'</li>\n",
       "\t<li>'of delayed gastric emptying from the cutaneous electrogastrograms (EGGs).'</li>\n",
       "\t<li>'Rojo-Alvarez, Garcia-Alberola, Artés-Rodriguez, and Arenal-Maiz apply'</li>\n",
       "\t<li>'SVMs, together with bootstrap resampling and principal component analysis,'</li>\n",
       "\t<li>'to tachycardia discrimination in implantable cardioverter defibrillators.'</li>\n",
       "</ol>\n",
       "</li>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'Preface Vil'</li>\n",
       "\t<li>'I would like to express my sincere appreciation to all authors and reviewers'</li>\n",
       "\t<li>'who have spent their precious time and efforts in making this book a reality.'</li>\n",
       "\t<li>'I wish to especially thank Professor Vojislav Kecman, who graciously took'</li>\n",
       "\t<li>'on the enormous task of writing a comprehensive introductory chapter, in'</li>\n",
       "\t<li>'addition to his other great contributions to this book. My gratitude also goes'</li>\n",
       "\t<li>'to Professor Janusz Kacprzyk and Dr. Thomas Ditzinger for their kindest'</li>\n",
       "\t<li>'support and help with this book.'</li>\n",
       "\t<li>'Singapore Lipo Wang'</li>\n",
       "\t<li>'January 2005'</li>\n",
       "</ol>\n",
       "</li>\n",
       "\t<li></li>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'Contents'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'Support Vector Machines — An Introduction'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'V. K€CMGN. 6. eee eee eee eee eeeeeee'</li>\n",
       "\t<li>'Multiple Model Estimation'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'for Nonlinear Classification'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'Y. Ma and V. Cherkassky .. 0.0.00. AY'</li>\n",
       "\t<li>'Componentwise Least Squares Support Vector Machines'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'Kk. Pelckmans, I. Goethals, J. De Brabanter, J.A.K. Suykens, and B.'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'De MOor .. ccc nee eee eee eee TT'</li>\n",
       "\t<li>'Active Support Vector Learning with Statistical Queries'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'P. Mitra, C.A. Murthy, and SK. Pal... eee ee ID'</li>\n",
       "\t<li>'Local Learning vs. Global Learning: An Introduction'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'to Maxi-Min Margin Machine'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'Kk. Huang, H. Yang, I. King, and M.R. Lyu ..... eee eee ee ee L18'</li>\n",
       "\t<li>'Active-Set Methods for Support Vector Machines'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'M. Vogt and V. Kecman ... 6... cece ee eee ee A 138'</li>\n",
       "\t<li>'Theoretical and Practical Model Selection Methods'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'for Support Vector Classifiers'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'D. Anguita, A. Boni, S. Ridella, FP. Rivieccio, and D. Sterpi...........159'</li>\n",
       "\t<li>'Adaptive Discriminant'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'and Quasiconformal Kernel Nearest Neighbor Classification'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'J. Peng, D.R. Heisterkamp, and H.K. Dat .. 1... ee ee LB'</li>\n",
       "\t<li>'Improving the Performance of the Support Vector Machine:'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'Two Geometrical Scaling Methods'</li>\n",
       "\t<li>''</li>\n",
       "\t<li>'P. Williams, S. Wu, and J. Feng 0.0... ccc cee eee ee 205'</li>\n",
       "</ol>\n",
       "</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'Support Vector Machines:'\n",
       "\\item 'Theory and Applications'\n",
       "\\item 'Lipo Wang'\n",
       "\\item '(ed.)'\n",
       "\\item ''\n",
       "\\item 'Springer, Berlin'\n",
       "\\item '2005'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'Preface'\n",
       "\\item ''\n",
       "\\item 'The support vector machine (SVM) is a supervised learning method that'\n",
       "\\item 'generates input-output mapping functions from a set of labeled training data.'\n",
       "\\item 'The mapping function can be either a classification function, i.e., the cate-'\n",
       "\\item 'gory of the input data, or a regression function. For classification, nonlinear'\n",
       "\\item 'kernel functions are often used to transform input data to a high-dimensional'\n",
       "\\item 'feature space in which the input data become more separable compared to'\n",
       "\\item 'the original input space. Maximum-margin hyperplanes are then created. ‘The'\n",
       "\\item 'model thus produced depends on only a subset of the training data near the'\n",
       "\\item 'class boundaries. Similarly, the model produced by Support Vector Regres-'\n",
       "\\item 'sion ignores any training data that is sufficiently close to the model prediction.'\n",
       "\\item 'SVMs are also said to belong to “kernel methods”.'\n",
       "\\item ''\n",
       "\\item 'In addition to its solid mathematical foundation in statistical learning'\n",
       "\\item 'theory, SVMs have demonstrated highly competitive performance in numerous'\n",
       "\\item 'real-world applications, such as bioinformatics, text mining, face recognition,'\n",
       "\\item 'and image processing, which has established SVMs as one of the state-of-'\n",
       "\\item 'the-art tools for machine learning and data mining, along with other soft'\n",
       "\\item 'computing techniques, e.g., neural networks and fuzzy systems.'\n",
       "\\item ''\n",
       "\\item 'This volume is composed of 20 chapters selected from the recent myriad'\n",
       "\\item 'of novel SVM applications, powerful SVM algorithms, as well as enlighten-'\n",
       "\\item 'ing theoretical analysis. Written by experts in their respective fields, the first'\n",
       "\\item '12 chapters concentrate on SVM theory, whereas the subsequent 8 chapters'\n",
       "\\item 'emphasize practical applications, although the “decision boundary” separat-'\n",
       "\\item 'ing these two categories is rather “fuzzy”.'\n",
       "\\item ''\n",
       "\\item 'Kecman first presents an introduction on the SVM, explaining the basic'\n",
       "\\item 'theory and implementation aspects. In the chapter contributed by Ma and'\n",
       "\\item 'Cherkassky, a novel approach to nonlinear classification using a collection of'\n",
       "\\item 'several simple (linear) classifiers is proposed based on a new formulation of'\n",
       "\\item 'the learning problem called multiple model estimation. Pelckmans, Goethals,'\n",
       "\\item 'De Brabanter, Suykens, and De Moor describe componentwise Least Squares'\n",
       "\\item 'Support Vector Machines (LS-SVMs) for the estimation of additive models'\n",
       "\\item 'consisting of a sum of nonlinear components.'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'VI Preface'\n",
       "\\item ''\n",
       "\\item 'Motivated by the statistical query model, Mitra, Murthy and Pal study an'\n",
       "\\item 'active learning strategy to solve the large quadratic programming problem of'\n",
       "\\item 'SVM design in data mining applications. Kaizhu Huang, Haiqin Yang, King,'\n",
       "\\item 'and Lyu propose a unifying theory of the Maxi-Min Margin Machine (M4)'\n",
       "\\item 'that subsumes the SVM, the minimax probability machine, and the linear'\n",
       "\\item 'discriminant analysis. Vogt and Kecman present an active-set algorithm for'\n",
       "\\item 'quadratic programming problems in SVMs, as an alternative to working-set'\n",
       "\\item '(decomposition) techniques, especially when the data set is not too large, the'\n",
       "\\item 'problem is ill-conditioned, or when high precision is needed.'\n",
       "\\item ''\n",
       "\\item 'Being aware of the abundance of methods for SVM model selection,'\n",
       "\\item 'Anguita, Boni, Ridella, Rivieccio, and Sterpi carefully analyze the most well-'\n",
       "\\item 'known methods and test some of them on standard benchmarks to evaluate'\n",
       "\\item 'their effectiveness. In an attempt to minimize bias, Peng, Heisterkamp, and'\n",
       "\\item 'Dai propose locally adaptive nearest neighbor classification methods by using'\n",
       "\\item 'locally linear SVMs and quasiconformal transformed kernels. Williams, Wu,'\n",
       "\\item 'and Feng discuss two geometric methods to improve SVM performance, i.e.,'\n",
       "\\item '(1) adapting kernels by magnifying the Riemannian metric in the neighbor-'\n",
       "\\item 'hood of the boundary, thereby increasing class separation, and (2) optimally'\n",
       "\\item 'locating the separating boundary, given that the distributions of data on either'\n",
       "\\item 'side may have different scales.'\n",
       "\\item ''\n",
       "\\item 'Song, Hu, and Xulei Yang derive a Kuhn-Tucker condition and a decom-'\n",
       "\\item 'position algorithm for robust SVMs to deal with overfitting in the presence of'\n",
       "\\item 'outliers. Lin and Sheng-de Wang design a fuzzy SVM with automatic deter-'\n",
       "\\item 'mination of the membership functions. Kecman, Te-Ming Huang, and Vogt'\n",
       "\\item 'present the latest developments and results of the Iterative Single Data Algo-'\n",
       "\\item 'rithm for solving large-scale problems.'\n",
       "\\item ''\n",
       "\\item 'Exploiting regularization and subspace decomposition techniques, Lu,'\n",
       "\\item 'Plataniotis, and Venetsanopoulos introduce a new kernel discriminant learn-'\n",
       "\\item 'ing method and apply the method to face recognition. Kwang In Kim, Jung,'\n",
       "\\item 'and Hang Joon Kim employ SVMs and neural networks for automobile li-'\n",
       "\\item 'cense plate localization, by classifying each pixel in the image into the object'\n",
       "\\item 'of interest or the background based on localized color texture patterns. Mat-'\n",
       "\\item 'tera discusses SVM applications in signal processing, especially the problem'\n",
       "\\item 'of digital channel equalization. Chu, Jin, and Lipo Wang use SVMs to solve'\n",
       "\\item 'two important problems in bioinformatics, i.e., cancer diagnosis based on mi-'\n",
       "\\item 'croarray gene expression data and protein secondary structure prediction.'\n",
       "\\item ''\n",
       "\\item 'Emulating the natural nose, Brezmes, Llobet, Al-Khalifa, Maldonado, and'\n",
       "\\item 'Gardner describe how SVMs are being evaluated in the gas sensor commu-'\n",
       "\\item 'nity to discriminate different blends of coffee, different types of vapors and'\n",
       "\\item 'nerve agents. Zhan presents an application of the SVM in inverse problems'\n",
       "\\item 'in ocean color remote sensing. Liang uses SVMs for non-invasive diagnosis'\n",
       "\\item 'of delayed gastric emptying from the cutaneous electrogastrograms (EGGs).'\n",
       "\\item 'Rojo-Alvarez, Garcia-Alberola, Artés-Rodriguez, and Arenal-Maiz apply'\n",
       "\\item 'SVMs, together with bootstrap resampling and principal component analysis,'\n",
       "\\item 'to tachycardia discrimination in implantable cardioverter defibrillators.'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'Preface Vil'\n",
       "\\item 'I would like to express my sincere appreciation to all authors and reviewers'\n",
       "\\item 'who have spent their precious time and efforts in making this book a reality.'\n",
       "\\item 'I wish to especially thank Professor Vojislav Kecman, who graciously took'\n",
       "\\item 'on the enormous task of writing a comprehensive introductory chapter, in'\n",
       "\\item 'addition to his other great contributions to this book. My gratitude also goes'\n",
       "\\item 'to Professor Janusz Kacprzyk and Dr. Thomas Ditzinger for their kindest'\n",
       "\\item 'support and help with this book.'\n",
       "\\item 'Singapore Lipo Wang'\n",
       "\\item 'January 2005'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\item \n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'Contents'\n",
       "\\item ''\n",
       "\\item 'Support Vector Machines — An Introduction'\n",
       "\\item ''\n",
       "\\item 'V. K€CMGN. 6. eee eee eee eee eeeeeee'\n",
       "\\item 'Multiple Model Estimation'\n",
       "\\item ''\n",
       "\\item 'for Nonlinear Classification'\n",
       "\\item ''\n",
       "\\item 'Y. Ma and V. Cherkassky .. 0.0.00. AY'\n",
       "\\item 'Componentwise Least Squares Support Vector Machines'\n",
       "\\item ''\n",
       "\\item 'Kk. Pelckmans, I. Goethals, J. De Brabanter, J.A.K. Suykens, and B.'\n",
       "\\item ''\n",
       "\\item 'De MOor .. ccc nee eee eee eee TT'\n",
       "\\item 'Active Support Vector Learning with Statistical Queries'\n",
       "\\item ''\n",
       "\\item 'P. Mitra, C.A. Murthy, and SK. Pal... eee ee ID'\n",
       "\\item 'Local Learning vs. Global Learning: An Introduction'\n",
       "\\item ''\n",
       "\\item 'to Maxi-Min Margin Machine'\n",
       "\\item ''\n",
       "\\item 'Kk. Huang, H. Yang, I. King, and M.R. Lyu ..... eee eee ee ee L18'\n",
       "\\item 'Active-Set Methods for Support Vector Machines'\n",
       "\\item ''\n",
       "\\item 'M. Vogt and V. Kecman ... 6... cece ee eee ee A 138'\n",
       "\\item 'Theoretical and Practical Model Selection Methods'\n",
       "\\item ''\n",
       "\\item 'for Support Vector Classifiers'\n",
       "\\item ''\n",
       "\\item 'D. Anguita, A. Boni, S. Ridella, FP. Rivieccio, and D. Sterpi...........159'\n",
       "\\item 'Adaptive Discriminant'\n",
       "\\item ''\n",
       "\\item 'and Quasiconformal Kernel Nearest Neighbor Classification'\n",
       "\\item ''\n",
       "\\item 'J. Peng, D.R. Heisterkamp, and H.K. Dat .. 1... ee ee LB'\n",
       "\\item 'Improving the Performance of the Support Vector Machine:'\n",
       "\\item ''\n",
       "\\item 'Two Geometrical Scaling Methods'\n",
       "\\item ''\n",
       "\\item 'P. Williams, S. Wu, and J. Feng 0.0... ccc cee eee ee 205'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. 1. 'Support Vector Machines:'\n",
       "2. 'Theory and Applications'\n",
       "3. 'Lipo Wang'\n",
       "4. '(ed.)'\n",
       "5. ''\n",
       "6. 'Springer, Berlin'\n",
       "7. '2005'\n",
       "\n",
       "\n",
       "\n",
       "2. 1. 'Preface'\n",
       "2. ''\n",
       "3. 'The support vector machine (SVM) is a supervised learning method that'\n",
       "4. 'generates input-output mapping functions from a set of labeled training data.'\n",
       "5. 'The mapping function can be either a classification function, i.e., the cate-'\n",
       "6. 'gory of the input data, or a regression function. For classification, nonlinear'\n",
       "7. 'kernel functions are often used to transform input data to a high-dimensional'\n",
       "8. 'feature space in which the input data become more separable compared to'\n",
       "9. 'the original input space. Maximum-margin hyperplanes are then created. ‘The'\n",
       "10. 'model thus produced depends on only a subset of the training data near the'\n",
       "11. 'class boundaries. Similarly, the model produced by Support Vector Regres-'\n",
       "12. 'sion ignores any training data that is sufficiently close to the model prediction.'\n",
       "13. 'SVMs are also said to belong to “kernel methods”.'\n",
       "14. ''\n",
       "15. 'In addition to its solid mathematical foundation in statistical learning'\n",
       "16. 'theory, SVMs have demonstrated highly competitive performance in numerous'\n",
       "17. 'real-world applications, such as bioinformatics, text mining, face recognition,'\n",
       "18. 'and image processing, which has established SVMs as one of the state-of-'\n",
       "19. 'the-art tools for machine learning and data mining, along with other soft'\n",
       "20. 'computing techniques, e.g., neural networks and fuzzy systems.'\n",
       "21. ''\n",
       "22. 'This volume is composed of 20 chapters selected from the recent myriad'\n",
       "23. 'of novel SVM applications, powerful SVM algorithms, as well as enlighten-'\n",
       "24. 'ing theoretical analysis. Written by experts in their respective fields, the first'\n",
       "25. '12 chapters concentrate on SVM theory, whereas the subsequent 8 chapters'\n",
       "26. 'emphasize practical applications, although the “decision boundary” separat-'\n",
       "27. 'ing these two categories is rather “fuzzy”.'\n",
       "28. ''\n",
       "29. 'Kecman first presents an introduction on the SVM, explaining the basic'\n",
       "30. 'theory and implementation aspects. In the chapter contributed by Ma and'\n",
       "31. 'Cherkassky, a novel approach to nonlinear classification using a collection of'\n",
       "32. 'several simple (linear) classifiers is proposed based on a new formulation of'\n",
       "33. 'the learning problem called multiple model estimation. Pelckmans, Goethals,'\n",
       "34. 'De Brabanter, Suykens, and De Moor describe componentwise Least Squares'\n",
       "35. 'Support Vector Machines (LS-SVMs) for the estimation of additive models'\n",
       "36. 'consisting of a sum of nonlinear components.'\n",
       "\n",
       "\n",
       "\n",
       "3. 1. 'VI Preface'\n",
       "2. ''\n",
       "3. 'Motivated by the statistical query model, Mitra, Murthy and Pal study an'\n",
       "4. 'active learning strategy to solve the large quadratic programming problem of'\n",
       "5. 'SVM design in data mining applications. Kaizhu Huang, Haiqin Yang, King,'\n",
       "6. 'and Lyu propose a unifying theory of the Maxi-Min Margin Machine (M4)'\n",
       "7. 'that subsumes the SVM, the minimax probability machine, and the linear'\n",
       "8. 'discriminant analysis. Vogt and Kecman present an active-set algorithm for'\n",
       "9. 'quadratic programming problems in SVMs, as an alternative to working-set'\n",
       "10. '(decomposition) techniques, especially when the data set is not too large, the'\n",
       "11. 'problem is ill-conditioned, or when high precision is needed.'\n",
       "12. ''\n",
       "13. 'Being aware of the abundance of methods for SVM model selection,'\n",
       "14. 'Anguita, Boni, Ridella, Rivieccio, and Sterpi carefully analyze the most well-'\n",
       "15. 'known methods and test some of them on standard benchmarks to evaluate'\n",
       "16. 'their effectiveness. In an attempt to minimize bias, Peng, Heisterkamp, and'\n",
       "17. 'Dai propose locally adaptive nearest neighbor classification methods by using'\n",
       "18. 'locally linear SVMs and quasiconformal transformed kernels. Williams, Wu,'\n",
       "19. 'and Feng discuss two geometric methods to improve SVM performance, i.e.,'\n",
       "20. '(1) adapting kernels by magnifying the Riemannian metric in the neighbor-'\n",
       "21. 'hood of the boundary, thereby increasing class separation, and (2) optimally'\n",
       "22. 'locating the separating boundary, given that the distributions of data on either'\n",
       "23. 'side may have different scales.'\n",
       "24. ''\n",
       "25. 'Song, Hu, and Xulei Yang derive a Kuhn-Tucker condition and a decom-'\n",
       "26. 'position algorithm for robust SVMs to deal with overfitting in the presence of'\n",
       "27. 'outliers. Lin and Sheng-de Wang design a fuzzy SVM with automatic deter-'\n",
       "28. 'mination of the membership functions. Kecman, Te-Ming Huang, and Vogt'\n",
       "29. 'present the latest developments and results of the Iterative Single Data Algo-'\n",
       "30. 'rithm for solving large-scale problems.'\n",
       "31. ''\n",
       "32. 'Exploiting regularization and subspace decomposition techniques, Lu,'\n",
       "33. 'Plataniotis, and Venetsanopoulos introduce a new kernel discriminant learn-'\n",
       "34. 'ing method and apply the method to face recognition. Kwang In Kim, Jung,'\n",
       "35. 'and Hang Joon Kim employ SVMs and neural networks for automobile li-'\n",
       "36. 'cense plate localization, by classifying each pixel in the image into the object'\n",
       "37. 'of interest or the background based on localized color texture patterns. Mat-'\n",
       "38. 'tera discusses SVM applications in signal processing, especially the problem'\n",
       "39. 'of digital channel equalization. Chu, Jin, and Lipo Wang use SVMs to solve'\n",
       "40. 'two important problems in bioinformatics, i.e., cancer diagnosis based on mi-'\n",
       "41. 'croarray gene expression data and protein secondary structure prediction.'\n",
       "42. ''\n",
       "43. 'Emulating the natural nose, Brezmes, Llobet, Al-Khalifa, Maldonado, and'\n",
       "44. 'Gardner describe how SVMs are being evaluated in the gas sensor commu-'\n",
       "45. 'nity to discriminate different blends of coffee, different types of vapors and'\n",
       "46. 'nerve agents. Zhan presents an application of the SVM in inverse problems'\n",
       "47. 'in ocean color remote sensing. Liang uses SVMs for non-invasive diagnosis'\n",
       "48. 'of delayed gastric emptying from the cutaneous electrogastrograms (EGGs).'\n",
       "49. 'Rojo-Alvarez, Garcia-Alberola, Artés-Rodriguez, and Arenal-Maiz apply'\n",
       "50. 'SVMs, together with bootstrap resampling and principal component analysis,'\n",
       "51. 'to tachycardia discrimination in implantable cardioverter defibrillators.'\n",
       "\n",
       "\n",
       "\n",
       "4. 1. 'Preface Vil'\n",
       "2. 'I would like to express my sincere appreciation to all authors and reviewers'\n",
       "3. 'who have spent their precious time and efforts in making this book a reality.'\n",
       "4. 'I wish to especially thank Professor Vojislav Kecman, who graciously took'\n",
       "5. 'on the enormous task of writing a comprehensive introductory chapter, in'\n",
       "6. 'addition to his other great contributions to this book. My gratitude also goes'\n",
       "7. 'to Professor Janusz Kacprzyk and Dr. Thomas Ditzinger for their kindest'\n",
       "8. 'support and help with this book.'\n",
       "9. 'Singapore Lipo Wang'\n",
       "10. 'January 2005'\n",
       "\n",
       "\n",
       "\n",
       "5. \n",
       "6. 1. 'Contents'\n",
       "2. ''\n",
       "3. 'Support Vector Machines — An Introduction'\n",
       "4. ''\n",
       "5. 'V. K€CMGN. 6. eee eee eee eee eeeeeee'\n",
       "6. 'Multiple Model Estimation'\n",
       "7. ''\n",
       "8. 'for Nonlinear Classification'\n",
       "9. ''\n",
       "10. 'Y. Ma and V. Cherkassky .. 0.0.00. AY'\n",
       "11. 'Componentwise Least Squares Support Vector Machines'\n",
       "12. ''\n",
       "13. 'Kk. Pelckmans, I. Goethals, J. De Brabanter, J.A.K. Suykens, and B.'\n",
       "14. ''\n",
       "15. 'De MOor .. ccc nee eee eee eee TT'\n",
       "16. 'Active Support Vector Learning with Statistical Queries'\n",
       "17. ''\n",
       "18. 'P. Mitra, C.A. Murthy, and SK. Pal... eee ee ID'\n",
       "19. 'Local Learning vs. Global Learning: An Introduction'\n",
       "20. ''\n",
       "21. 'to Maxi-Min Margin Machine'\n",
       "22. ''\n",
       "23. 'Kk. Huang, H. Yang, I. King, and M.R. Lyu ..... eee eee ee ee L18'\n",
       "24. 'Active-Set Methods for Support Vector Machines'\n",
       "25. ''\n",
       "26. 'M. Vogt and V. Kecman ... 6... cece ee eee ee A 138'\n",
       "27. 'Theoretical and Practical Model Selection Methods'\n",
       "28. ''\n",
       "29. 'for Support Vector Classifiers'\n",
       "30. ''\n",
       "31. 'D. Anguita, A. Boni, S. Ridella, FP. Rivieccio, and D. Sterpi...........159'\n",
       "32. 'Adaptive Discriminant'\n",
       "33. ''\n",
       "34. 'and Quasiconformal Kernel Nearest Neighbor Classification'\n",
       "35. ''\n",
       "36. 'J. Peng, D.R. Heisterkamp, and H.K. Dat .. 1... ee ee LB'\n",
       "37. 'Improving the Performance of the Support Vector Machine:'\n",
       "38. ''\n",
       "39. 'Two Geometrical Scaling Methods'\n",
       "40. ''\n",
       "41. 'P. Williams, S. Wu, and J. Feng 0.0... ccc cee eee ee 205'\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "[1] \"Support Vector Machines:\" \"Theory and Applications\" \n",
       "[3] \"Lipo Wang\"                \"(ed.)\"                   \n",
       "[5] \"\"                         \"Springer, Berlin\"        \n",
       "[7] \"2005\"                    \n",
       "\n",
       "[[2]]\n",
       " [1] \"Preface\"                                                                           \n",
       " [2] \"\"                                                                                  \n",
       " [3] \"The support vector machine (SVM) is a supervised learning method that\"             \n",
       " [4] \"generates input-output mapping functions from a set of labeled training data.\"     \n",
       " [5] \"The mapping function can be either a classification function, i.e., the cate-\"     \n",
       " [6] \"gory of the input data, or a regression function. For classification, nonlinear\"   \n",
       " [7] \"kernel functions are often used to transform input data to a high-dimensional\"     \n",
       " [8] \"feature space in which the input data become more separable compared to\"           \n",
       " [9] \"the original input space. Maximum-margin hyperplanes are then created. ‘The\"       \n",
       "[10] \"model thus produced depends on only a subset of the training data near the\"        \n",
       "[11] \"class boundaries. Similarly, the model produced by Support Vector Regres-\"         \n",
       "[12] \"sion ignores any training data that is sufficiently close to the model prediction.\"\n",
       "[13] \"SVMs are also said to belong to “kernel methods”.\"                                 \n",
       "[14] \"\"                                                                                  \n",
       "[15] \"In addition to its solid mathematical foundation in statistical learning\"          \n",
       "[16] \"theory, SVMs have demonstrated highly competitive performance in numerous\"         \n",
       "[17] \"real-world applications, such as bioinformatics, text mining, face recognition,\"   \n",
       "[18] \"and image processing, which has established SVMs as one of the state-of-\"          \n",
       "[19] \"the-art tools for machine learning and data mining, along with other soft\"         \n",
       "[20] \"computing techniques, e.g., neural networks and fuzzy systems.\"                    \n",
       "[21] \"\"                                                                                  \n",
       "[22] \"This volume is composed of 20 chapters selected from the recent myriad\"            \n",
       "[23] \"of novel SVM applications, powerful SVM algorithms, as well as enlighten-\"         \n",
       "[24] \"ing theoretical analysis. Written by experts in their respective fields, the first\"\n",
       "[25] \"12 chapters concentrate on SVM theory, whereas the subsequent 8 chapters\"          \n",
       "[26] \"emphasize practical applications, although the “decision boundary” separat-\"       \n",
       "[27] \"ing these two categories is rather “fuzzy”.\"                                       \n",
       "[28] \"\"                                                                                  \n",
       "[29] \"Kecman first presents an introduction on the SVM, explaining the basic\"            \n",
       "[30] \"theory and implementation aspects. In the chapter contributed by Ma and\"           \n",
       "[31] \"Cherkassky, a novel approach to nonlinear classification using a collection of\"    \n",
       "[32] \"several simple (linear) classifiers is proposed based on a new formulation of\"     \n",
       "[33] \"the learning problem called multiple model estimation. Pelckmans, Goethals,\"       \n",
       "[34] \"De Brabanter, Suykens, and De Moor describe componentwise Least Squares\"           \n",
       "[35] \"Support Vector Machines (LS-SVMs) for the estimation of additive models\"           \n",
       "[36] \"consisting of a sum of nonlinear components.\"                                      \n",
       "\n",
       "[[3]]\n",
       " [1] \"VI Preface\"                                                                      \n",
       " [2] \"\"                                                                                \n",
       " [3] \"Motivated by the statistical query model, Mitra, Murthy and Pal study an\"        \n",
       " [4] \"active learning strategy to solve the large quadratic programming problem of\"    \n",
       " [5] \"SVM design in data mining applications. Kaizhu Huang, Haiqin Yang, King,\"        \n",
       " [6] \"and Lyu propose a unifying theory of the Maxi-Min Margin Machine (M4)\"           \n",
       " [7] \"that subsumes the SVM, the minimax probability machine, and the linear\"          \n",
       " [8] \"discriminant analysis. Vogt and Kecman present an active-set algorithm for\"      \n",
       " [9] \"quadratic programming problems in SVMs, as an alternative to working-set\"        \n",
       "[10] \"(decomposition) techniques, especially when the data set is not too large, the\"  \n",
       "[11] \"problem is ill-conditioned, or when high precision is needed.\"                   \n",
       "[12] \"\"                                                                                \n",
       "[13] \"Being aware of the abundance of methods for SVM model selection,\"                \n",
       "[14] \"Anguita, Boni, Ridella, Rivieccio, and Sterpi carefully analyze the most well-\"  \n",
       "[15] \"known methods and test some of them on standard benchmarks to evaluate\"          \n",
       "[16] \"their effectiveness. In an attempt to minimize bias, Peng, Heisterkamp, and\"     \n",
       "[17] \"Dai propose locally adaptive nearest neighbor classification methods by using\"   \n",
       "[18] \"locally linear SVMs and quasiconformal transformed kernels. Williams, Wu,\"       \n",
       "[19] \"and Feng discuss two geometric methods to improve SVM performance, i.e.,\"        \n",
       "[20] \"(1) adapting kernels by magnifying the Riemannian metric in the neighbor-\"       \n",
       "[21] \"hood of the boundary, thereby increasing class separation, and (2) optimally\"    \n",
       "[22] \"locating the separating boundary, given that the distributions of data on either\"\n",
       "[23] \"side may have different scales.\"                                                 \n",
       "[24] \"\"                                                                                \n",
       "[25] \"Song, Hu, and Xulei Yang derive a Kuhn-Tucker condition and a decom-\"            \n",
       "[26] \"position algorithm for robust SVMs to deal with overfitting in the presence of\"  \n",
       "[27] \"outliers. Lin and Sheng-de Wang design a fuzzy SVM with automatic deter-\"        \n",
       "[28] \"mination of the membership functions. Kecman, Te-Ming Huang, and Vogt\"           \n",
       "[29] \"present the latest developments and results of the Iterative Single Data Algo-\"  \n",
       "[30] \"rithm for solving large-scale problems.\"                                         \n",
       "[31] \"\"                                                                                \n",
       "[32] \"Exploiting regularization and subspace decomposition techniques, Lu,\"            \n",
       "[33] \"Plataniotis, and Venetsanopoulos introduce a new kernel discriminant learn-\"     \n",
       "[34] \"ing method and apply the method to face recognition. Kwang In Kim, Jung,\"        \n",
       "[35] \"and Hang Joon Kim employ SVMs and neural networks for automobile li-\"            \n",
       "[36] \"cense plate localization, by classifying each pixel in the image into the object\"\n",
       "[37] \"of interest or the background based on localized color texture patterns. Mat-\"   \n",
       "[38] \"tera discusses SVM applications in signal processing, especially the problem\"    \n",
       "[39] \"of digital channel equalization. Chu, Jin, and Lipo Wang use SVMs to solve\"      \n",
       "[40] \"two important problems in bioinformatics, i.e., cancer diagnosis based on mi-\"   \n",
       "[41] \"croarray gene expression data and protein secondary structure prediction.\"       \n",
       "[42] \"\"                                                                                \n",
       "[43] \"Emulating the natural nose, Brezmes, Llobet, Al-Khalifa, Maldonado, and\"         \n",
       "[44] \"Gardner describe how SVMs are being evaluated in the gas sensor commu-\"          \n",
       "[45] \"nity to discriminate different blends of coffee, different types of vapors and\"  \n",
       "[46] \"nerve agents. Zhan presents an application of the SVM in inverse problems\"       \n",
       "[47] \"in ocean color remote sensing. Liang uses SVMs for non-invasive diagnosis\"       \n",
       "[48] \"of delayed gastric emptying from the cutaneous electrogastrograms (EGGs).\"       \n",
       "[49] \"Rojo-Alvarez, Garcia-Alberola, Artés-Rodriguez, and Arenal-Maiz apply\"           \n",
       "[50] \"SVMs, together with bootstrap resampling and principal component analysis,\"      \n",
       "[51] \"to tachycardia discrimination in implantable cardioverter defibrillators.\"       \n",
       "\n",
       "[[4]]\n",
       " [1] \"Preface Vil\"                                                                   \n",
       " [2] \"I would like to express my sincere appreciation to all authors and reviewers\"  \n",
       " [3] \"who have spent their precious time and efforts in making this book a reality.\" \n",
       " [4] \"I wish to especially thank Professor Vojislav Kecman, who graciously took\"     \n",
       " [5] \"on the enormous task of writing a comprehensive introductory chapter, in\"      \n",
       " [6] \"addition to his other great contributions to this book. My gratitude also goes\"\n",
       " [7] \"to Professor Janusz Kacprzyk and Dr. Thomas Ditzinger for their kindest\"       \n",
       " [8] \"support and help with this book.\"                                              \n",
       " [9] \"Singapore Lipo Wang\"                                                           \n",
       "[10] \"January 2005\"                                                                  \n",
       "\n",
       "[[5]]\n",
       "character(0)\n",
       "\n",
       "[[6]]\n",
       " [1] \"Contents\"                                                                   \n",
       " [2] \"\"                                                                           \n",
       " [3] \"Support Vector Machines — An Introduction\"                                  \n",
       " [4] \"\"                                                                           \n",
       " [5] \"V. K\\200CMGN. 6. eee eee eee eee eeeeeee\"                                      \n",
       " [6] \"Multiple Model Estimation\"                                                  \n",
       " [7] \"\"                                                                           \n",
       " [8] \"for Nonlinear Classification\"                                               \n",
       " [9] \"\"                                                                           \n",
       "[10] \"Y. Ma and V. Cherkassky .. 0.0.00. AY\"                                      \n",
       "[11] \"Componentwise Least Squares Support Vector Machines\"                        \n",
       "[12] \"\"                                                                           \n",
       "[13] \"Kk. Pelckmans, I. Goethals, J. De Brabanter, J.A.K. Suykens, and B.\"        \n",
       "[14] \"\"                                                                           \n",
       "[15] \"De MOor .. ccc nee eee eee eee TT\"                                          \n",
       "[16] \"Active Support Vector Learning with Statistical Queries\"                    \n",
       "[17] \"\"                                                                           \n",
       "[18] \"P. Mitra, C.A. Murthy, and SK. Pal... eee ee ID\"                            \n",
       "[19] \"Local Learning vs. Global Learning: An Introduction\"                        \n",
       "[20] \"\"                                                                           \n",
       "[21] \"to Maxi-Min Margin Machine\"                                                 \n",
       "[22] \"\"                                                                           \n",
       "[23] \"Kk. Huang, H. Yang, I. King, and M.R. Lyu ..... eee eee ee ee L18\"          \n",
       "[24] \"Active-Set Methods for Support Vector Machines\"                             \n",
       "[25] \"\"                                                                           \n",
       "[26] \"M. Vogt and V. Kecman ... 6... cece ee eee ee A 138\"                        \n",
       "[27] \"Theoretical and Practical Model Selection Methods\"                          \n",
       "[28] \"\"                                                                           \n",
       "[29] \"for Support Vector Classifiers\"                                             \n",
       "[30] \"\"                                                                           \n",
       "[31] \"D. Anguita, A. Boni, S. Ridella, FP. Rivieccio, and D. Sterpi...........159\"\n",
       "[32] \"Adaptive Discriminant\"                                                      \n",
       "[33] \"\"                                                                           \n",
       "[34] \"and Quasiconformal Kernel Nearest Neighbor Classification\"                  \n",
       "[35] \"\"                                                                           \n",
       "[36] \"J. Peng, D.R. Heisterkamp, and H.K. Dat .. 1... ee ee LB\"                   \n",
       "[37] \"Improving the Performance of the Support Vector Machine:\"                   \n",
       "[38] \"\"                                                                           \n",
       "[39] \"Two Geometrical Scaling Methods\"                                            \n",
       "[40] \"\"                                                                           \n",
       "[41] \"P. Williams, S. Wu, and J. Feng 0.0... ccc cee eee ee 205\"                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text2 <- strsplit(text, \"\\n\")\n",
    "head(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
